---
title: "Introduction to Machine Learning - Lab 3"
author: "Gustav Sternel√∂v"
date: "Tuesday, November 03, 2015"
output: pdf_document
---

## Assignment 1
In the first assignment a data material named australian crabs is analysed. It contanis information about measurements of the frontal lobe, rear witdh etc. for 200 crabs. 

### 1.1
A scatterplot visualizing the the carapace length versus rear width.  

```{r, echo=FALSE, warning=FALSE}
aussieCrab <- read.csv("australian-crabs.csv", sep=",", header = TRUE)

library(ggplot2)
ggplot(aussieCrab, aes(y=RW, x=CL)) + geom_point(aes(color=sex), size=3)

```

### 1.2



## Assignment 2

### 2.1

```{r, echo=FALSE}
creditScore <- read.csv("creditscoring.csv", sep=";", header = TRUE) 

# create train set
n=dim(creditScore)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=creditScore[id,]
test=creditScore[-id,]

# create valid and test
n=dim(test)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
valid=test[id,]
test=test[-id,]
```


### 2.2

```{r, echo=FALSE, warning=FALSE}
library(tree)
# a) - deviance
deviFit <- tree(good_bad~., data=train, split="deviance")
#plot(deviFit)
#text(deviFit, pretty=0)
#deviFit
summary(deviFit)
# b) gini index
giniFit <- tree(good_bad~., data=train, split="gini")
summary(giniFit)

# Lower misclassification for deviance
```


### 2.3

```{r, echo=FALSE}
trainScore <- rep(0,15)
testScore <- rep(0,15)

for (i in 2:15){
  prunedTree <- prune.tree(deviFit, best=i)
  pred <- predict(prunedTree, newdata=valid,
               type="tree")
  trainScore[i] <- deviance(prunedTree)
  testScore[i] <- deviance(pred)
}

plot(2:15, trainScore[2:15], type="b", col="red", ylim=c(250,550))
points(2:15, testScore[2:15], type="b", col="blue")
# Which is the optimal number of leaves?
# A try with optimal number of leaves set to 12
finalTree <- prune.tree(deviFit, best=12)
# The variables used
summary(finalTree)
# Model tested on test data
NewFit <- predict(finalTree, newdata=test, type="class")
table(test$good_bad, NewFit)
```


### 2.4

```{r, echo=FALSE, warning=FALSE}
library(e1071)
# bayes classifier based on training data
bayesFit <- naiveBayes(good_bad~., data=train)
# Fitted valuse for training and test based on classifier
bayesPredTrain <- predict(bayesFit, newdata=train)
bayesPredTest <- predict(bayesFit, newdata=test)
# confusion matrices for train and test
bayesTrain <-table(bayesPredTrain, train$good_bad)
bayesTest <- table(bayesPredTest, test$good_bad)
table(bayesPredTrain, train$good_bad)
table(bayesPredTest, test$good_bad)
```

### 2.5
Repeating 2.4 but with a defined loss matrix. 
```{r, echo=FALSE}
# Repeating 2.4 but with a defined loss matrix. 
lossMat <- matrix(c(0,10,1,0), ncol=2)

lossMat * bayesTrain
lossMat * bayesTest

```
If I set the zeros in the loss matrix to one, will the right confusion matrix be obtained? Like this
```{r, echo=FALSE}
# If I set the zeros in the loss matrix to one, will the right confusion matrix be
# obtained? Like this
lossMat2 <- matrix(c(1,10,1,1), ncol=2)

lossMat2 * bayesTrain
lossMat2 * bayesTest
```

