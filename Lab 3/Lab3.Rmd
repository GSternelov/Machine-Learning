---
title: "Introduction to Machine Learning - Lab 3"
author: "Gustav Sternel√∂v"
date: "Tuesday, November 03, 2015"
output:
  pdf_document:
    fig_height: 4
    fig_width: 7
---

## Assignment 1
In the first assignment a data material named australian crabs is analysed. It contanis information about measurements of the frontal lobe, rear witdh etc. for 200 crabs. 

### 1.1
A scatterplot visualizing the carapace length versus rear width.  

```{r, echo=FALSE, warning=FALSE}
aussieCrab <- read.csv("australian-crabs.csv", sep=",", header = TRUE)

library(ggplot2)
ggplot(aussieCrab, aes(y=CL, x=RW)) + geom_point(aes(color=sex), size=3)

```


For values of RW higher than 12.5 and values of CL higher than 30 this data should be easy to classify by linear discriminant analysis. For lower values the obseervations lies closer and it will problaby be harder to find a linear decision boundary that classifies all those observations correctly. 

### 1.2

```{r, echo=FALSE}
# 1.2

male_data <- subset(aussieCrab, sex=="Male")[5:6]
female_data <- subset(aussieCrab, sex=="Female")[5:6]

male_vec <- c(mean(male_data[,1]), mean(male_data[,2]))
female_vec <- c(mean(female_data[,1]), mean(female_data[,2]))

male_cov <- cov(male_data)
female_cov <- cov(female_data)

covHat <- matrix(100/200 * male_cov + 100/200 * female_cov, ncol=2)

# prop priors
prior_male <- 100/200
prior_female <- 100/200

W_male <- (as.matrix(aussieCrab[, 5:6]))%*% solve(covHat) %*% as.matrix(male_vec) 
Wo_male <-  (0.5 *  t(as.matrix(male_vec)) %*% solve(covHat) %*% as.matrix(male_vec) +
  log(prior_male))
disc_male <- W_male - as.vector(Wo_male)

W_female <- (as.matrix(aussieCrab[, 5:6]))%*% solve(covHat)%*% as.matrix(female_vec) 
Wo_female <- (0.5 *  t(as.matrix(female_vec)) %*% solve(covHat) %*% as.matrix(female_vec) +
   log(prior_female))
disc_female <-W_female - as.vector(Wo_female)

classif <- 0
for(i in 1:200){
  if(disc_male[i] > disc_female[i]){
    classif[i] = "male"
  }else{
    classif[i] = "female"
  }
}

```

**extract the discriminant functions**  
The respective discriminant function for males and females looks as follows:
delta_male (x) = $$x^T\sum^-1$$



And the decision boundary is given by the this expression : CL = 2.91RW - 5.06  

### 1.3
In the following plot the observed values of RW and CL are visualized. Colours are given after the classification label and shape after the true class for the observations. The red line gives the decision boundary.  

```{r, echo=FALSE}
#table(classif, aussieCrab$sex)
lda_class <- data.frame(aussieCrab, classif)

ggplot(lda_class, aes(y=CL, x=RW)) + 
  geom_point(aes(color=classif,shape=sex), size=4) +
  geom_abline(intercept = -5.06, slope=2.91, colour="red")

```

The fit is considered to be pretty god. Only 7 of 200 observations is classified wrongly, and as expected most of these observations are find at the bottom left of the plot. 

### 1.4

Another method to classify the observations are by a logistic regression model. In the graph below the classifications are plotted in the same way as in 1.3.  

```{r, echo=FALSE, warning=FALSE}
logi_class <- glm(sex~RW+CL,data=aussieCrab, family=binomial())
#summary(logi_class)
#exp(coef(logi_class))
pred_logi <- data.frame(aussieCrab$RW,aussieCrab$CL,aussieCrab$sex , predict(logi_class, type="response"))

for (i in 1:length(pred_logi[,4])){
if(pred_logi[,4][i] <0.5){
  pred_logi[,4][i] = 0
}else{
  pred_logi[,4][i] = 1
} }
  
ggplot(pred_logi, aes(y=aussieCrab.CL, x=aussieCrab.RW)) + 
  geom_point(aes(color=pred_logi[,4],shape=aussieCrab.sex), size=4) +
  geom_abline(intercept = -2.94, slope=2.713, colour="red")

#table(pred_logi[,4], pred_logi$aussieCrab.sex)
```
 
The decision boundary for the logistic regression model: CL = 2.713RW - 2.94  
The obtained result with logistic regression is very similar to the result given with the *lda* model. For both models seven observations has been misclassified, and even though the decision boundaries are different it is first and foremost at the bottom left of the graph the misclassified observations can be found. A comparsion of the lines for the two models decision boundaries gives that the line for the logistic model is slightly shifted to the left compared to the line for the *lda* model. 

## Assignment 2
For the second assignment the analyzed data contains information about how well costumers have managed their loans. The costumers are divided into two classes, good or bad borrowers, and there is 19 different potential features that can be used for classifying.

### 2.1
To start with, data is splitted into train, validation and test. 
```{r, echo=FALSE}
creditScore <- read.csv("creditscoring.csv", sep=";", header = TRUE) 

# create train set
n=dim(creditScore)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=creditScore[id,]
test=creditScore[-id,]

# create valid and test
n=dim(test)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
valid=test[id,]
test=test[-id,]
```


### 2.2

```{r, echo=FALSE, warning=FALSE}
library(tree)
# a) - deviance
# train
deviFit <- tree(good_bad~., data=train, split="deviance")
#plot(deviFit)
#text(deviFit, pretty=0)
#deviFit
devi_summary <- summary(deviFit)
devi_summary$misclass[1] / devi_summary$misclass[2]


# b) gini index
# train
giniFit <- tree(good_bad~., data=train, split="gini")
gini_summary <- summary(giniFit)
gini_summary$misclass[1] / gini_summary$misclass[2]

# Lower misclassification for deviance
```
**report the misclassification rates for the training and test data. Choose the measure providing the better results for the following steps.**  
For training data with deviance as 


### 2.3

```{r, echo=FALSE}
trainScore <- rep(0,15)
testScore <- rep(0,15)

for (i in 2:15){
  prunedTree <- prune.tree(deviFit, best=i)
  pred <- predict(prunedTree, newdata=valid,
               type="tree")
  trainScore[i] <- deviance(prunedTree)
  testScore[i] <- deviance(pred)
}

plot(2:15, trainScore[2:15], type="b", col="red", ylim=c(250,550))
points(2:15, testScore[2:15], type="b", col="blue")
# Which is the optimal number of leaves?
# A try with optimal number of leaves set to 12
finalTree <- prune.tree(deviFit, best=12)
# The variables used
summary(finalTree)
# Model tested on test data
NewFit <- predict(finalTree, newdata=test, type="class")
table(test$good_bad, NewFit)
```


### 2.4

```{r, echo=FALSE, warning=FALSE}
library(e1071)
# bayes classifier based on training data
bayesFit <- naiveBayes(good_bad~., data=train)
# Fitted valuse for training and test based on classifier
bayesPredTrain <- predict(bayesFit, newdata=train)
bayesPredTest <- predict(bayesFit, newdata=test)
# confusion matrices for train and test
bayesTrain <-table(bayesPredTrain, train$good_bad)
bayesTest <- table(bayesPredTest, test$good_bad)
table(bayesPredTrain, train$good_bad)
table(bayesPredTest, test$good_bad)
```

### 2.5
Repeating 2.4 but with a defined loss matrix. 
```{r, echo=FALSE}
# Repeating 2.4 but with a defined loss matrix. 
raws <- data.frame(predict(bayesFit, newdata=train, type="raw"))

preds <- 0
for (i in 1:500){
  if((raws[i,1]/raws[i,2]) > 0.1){
    preds[i] = "bad"
  }else{
    preds[i] ="good"
  }  
}

table(train$good_bad, preds)

rawTest <- predict(bayesFit, newdata=test, type="raw")
preds <- 0
for (i in 1:250){
  if((rawTest[i,1]/rawTest[i,2]) > 0.1){
    preds[i] = "bad"
  }else{
    preds[i] ="good"
  }  
}

table(test$good_bad, preds)

```

