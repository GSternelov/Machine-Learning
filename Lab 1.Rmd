---
title: "Introduction to machine learning - Lab 1"
author: "Gustav Sternel√∂v"
date: "Thursday, October 22, 2015"
output: html_document
---

## Assignment 1

### 1.1
In the first step I divide the data into a train and a test data set. Each data set conatins 50 percent of the original data set.
```{r, echo=FALSE}
dat <- read.csv("spambase.csv", sep=";", header = TRUE)
dat <- as.matrix(dat)

n=dim(dat)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=dat[id,]
test=dat[-id,]
```

### 1.2
Create the knearest function. 
```{r, echo=FALSE}
knearest <- function(data, K,p=0.5 ,newdata){
  Predict <- 0
  ProbOne <- 0
  ProbZero <- 0
  
  x <- as.matrix(data[,1:48])
  y <- as.matrix(newdata[,1:48])
  x_transformed <- t(apply(x, 1, function(x) x / sqrt(sum(x^2))))
  x_transformed[is.na(x_transformed)] <- 0
  y_transformed <- t(apply(y, 1, function(y) y / sqrt(sum(y^2))))
  y_transformed[is.na(y_transformed)] <- 0
  
  numerator <- (x_transformed) %*% t(y_transformed)
  d_xy <- data.frame(1 - numerator)

  for(i in 1:length(d_xy)){
    LabelsTrain <- data[order(d_xy[,i]), 49][1:K]
    ProbOne[i] <- mean(LabelsTrain)
    ProbZero[i] <- 1-ProbOne[i]
    if(ProbOne[i] > p){
      Predict[i] = 1
    }else{
      Predict[i] = 0
    }
  }
newdata <- data.frame(newdata)
newdata$predict <- Predict
newdata$prob_one <- ProbOne
newdata$prob_zero <- ProbZero
return(newdata)
}
```

### 1.3
Classifying data with k nearest neigbours set to 5 and the classification principle...
The confusion matrix is then obtained for both train and test data.
For train data: 
```{r, echo=FALSE}
testish <- knearest(train, 5, 0.5, train)
table(testish$predict, testish$Spam)
1- sum(diag(table(testish$predict, testish$Spam)))/length(testish$predict)
```
For test data:
```{r, echo=FALSE}
testish2 <- knearest(train, 5, 0.5, test)
table(testish2$predict, testish2$Spam)
1- sum(diag(table(testish2$predict, testish2$Spam)))/length(testish2$predict)
```


The misclassification rate for the train data is... and for test data the misclassification rate is...

### 1.4
With a K set to 1 the following confusion matrices and misclassification rates are obtained.

For train data: 
```{r, echo=FALSE}
testish <- knearest(train, 1, 0.5, train)
table(testish$predict, testish$Spam)
1- sum(diag(table(testish$predict, testish$Spam)))/length(testish$predict)
```
For test data:
```{r, echo=FALSE}
testish2 <- knearest(train, 1, 0.5, test)
table(testish2$predict, testish2$Spam)
1- sum(diag(table(testish2$predict, testish2$Spam)))/length(testish2$predict)
```


### 1.5
Classify the test set with the function kknn from the package kknn. 
The confusion matrix for K equal to 5:

```{r, echo=FALSE, warning=FALSE}
library(kknn)
train <- data.frame(train)
test <- data.frame(test)
test_kknn <- kknn(Spam~., train, test, k=5)
fitted_v <- test_kknn$fitted.values
for (i in 1:length(fitted_v)){
if(fitted_v[i] > 0.5 ){
  fitted_v[i] = 1
}else{
  fitted_v[i] = 0
}
}
conf_mat <- data.frame(cbind(fitted_v, test[,49]))
table(conf_mat[,2], conf_mat[,1])
```

This method gives a misclassification rate of...

Compared to the results given in 1.3 and 1.4...

### 1.6
```{r, echo=FALSE}
test_kknn <- kknn(Spam~., train, test, k=5)
fitted_v <- test_kknn$fitted.values
phis <- seq(0.05, 0.95, by=0.05)
h <- 0
classify <- matrix(ncol=19, nrow=2301)
predic <- 0

for (j in phis){
  for (i in 1:length(fitted_v)){
    if(fitted_v[i] > j ){
      predic[i] = 1
    }else{
      predic[i] = 0
    }
  }
  h <- h+1
  classify[,h] <- predic
}

# For knearest()
Knear_class <- matrix(ncol=19, nrow=2301)
h <- 0
for (j in phis){
  testish <- knearest(train, 5, p=j, test)
  h <- h+1
  Knear_class[,h] <- testish$predict
}
```





## Assignment 2
### 2.1
Reads in data. 
```{r,echo=FALSE}
machine <- read.csv("machines.csv", sep=";", header = TRUE)
```

### 2.2
The distribution for x is the exponential distribution. 
The dependence of log-likelihood on theta is examined with the graph below. The x-axis represents the values in the sequence 0.01 to 5 by steps of 0.01. 


```{r, echo=FALSE}
n <- length(machine[,1])
theta <- seq(0.01, 5, by=0.01)

loglike_Theta <- n * log(theta) - theta *sum(machine[,1])
plot(loglike_Theta, type="l")

theta[which(loglike_Theta==max(loglike_Theta))]
```

As can be seen by the graph the log-likelihood increases quickly for the first 50 values (theta from 0.01 to 0.5). Thereafter the log-likelihood increases a bit slower until it reaches it max value at theta equal to 1.13, which is the maximum likelihood value of theta. For theta values higher than 1.13 the log-likelihood decreases slowly. 

### 2.3

In this exercise 2.2 is repeated, with the difference that only the six first observations from the machine data is used. 
```{r, echo=FALSE}
machine_six <- data.frame(machine[1:6, 1])
n_six <- length(machine_six[,1])
theta <- seq(0.01, 5, by=0.01)

loglike_ThetaSix <- n_six * log(theta) + -theta *sum(machine_six[,1])
theta[which(loglike_ThetaSix==max(loglike_ThetaSix))]

par(mfrow=c(1,2))
plot(loglike_Theta, type="l")
plot(loglike_ThetaSix, type="l")
par(mfrow=c(1,1))
```

The maximum likelihood is more reliable for larger data sets. 

### 2.4

```{r, echo=FALSE}
lambda <- 0.5
theta <- seq(0.01, 3, by=0.01)
x <- machine[,1]

l_theta <- n*log(lambda) + n*log(theta) - n*theta - theta * sum(x)
plot(l_theta, type="l")
theta[which(l_theta==max(l_theta))]
```

### 2.5

The theta value of 0.53 is in this exercise used to simulate 50 new values from a the exponential distribution. 

```{r,echo=FALSE}
R_exp <-rexp(50, 0.53)
par(mfrow=c(1,2))
hist(machine[,1])
hist(R_exp)
par(mfrow=c(1,1))
```
