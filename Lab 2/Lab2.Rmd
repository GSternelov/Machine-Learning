---
title: "Introduction to Machine Learning - Lab 2"
author: "Gustav Sternel√∂v"
date: "Thursday, October 29, 2015"
output:
  pdf_document:
    fig_height: 4
    fig_width: 7
---

## Assignment 1
### 1.1
The function *ridgereg_nfoldCV* which does ridge regression by using n-fold cross validation is implemented at the first step of assignment 1. The R-code used to create the function can be seen below. 
```{r 1.1,eval=TRUE}
ridgereg_nfoldCV <- function(x, y, lambda, nfolds){
  # Create the folds and initialize CV vector
  n <- length(y)
  seques <- floor(n/nfolds)
  reps <- n%%nfolds
  groups <- rep(seq(1,nfolds, 1), seques)
  end_values <- rep(nfolds, reps)
  folds <- c(groups, end_values)
  folds <- sort(folds) 
  
  x <- cbind(rep(1, nrow(x)), x)
  CV <- 0
  for (i in 1:nfolds){
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData_x <- x[testIndexes, ]
    testData_y <- y[testIndexes]
    trainData_x <- x[-testIndexes, ]
    trainData_y <- y[-testIndexes]
    
    # Perform ridge regression on train data
    x_t <- t(trainData_x)
    I <- diag(ncol(trainData_x))
    BetaRidge <- solve(x_t %*% trainData_x + lambda * I) %*% x_t %*% trainData_y
    #y_hat <- trainData_x %*% BetaRidge
    # Test regression on test data and compare with true values for test data
    y_hat_test <- testData_x %*% BetaRidge
    # Calculates CV
    CV[i] <- sum((testData_y - y_hat_test)^2)
  }
  CV_score <- (1/nfolds * sum(CV))
  return(CV_score)
}

```

### 1.2
The function implemented in 1.2 is tested with the longley data set. Number of folds is set to 10 and the value for lambda goes from 1 to 7, by 1. The given values for the CV score is shown in the following table and it can be seen that the CV score decreases for higher values of lambda. 
```{r 1.2, echo=FALSE, eval=TRUE}
library(MASS)
data(longley)
data <- longley

longley.x <- data.matrix(longley[, 1:6])
longley.y <- longley[, "Employed"]

longley.y <- longley.y - mean(longley.y)
for(i in 1:6){
  longley.x[,i] <- longley.x[,i] - mean(longley.x[,i]) 
  
}
for (i in 1:7){
  print(ridgereg_nfoldCV(longley.x, longley.y, lambda=i, nfolds=5))
}
```

## Assignment 2
### 2.1
```{r, echo=FALSE}
tecator <- read.csv("tecator.csv", sep=";", header = TRUE)

plot(tecator$Protein, tecator$Moisture)
```
To describe moisture with protein data with a linear model may be an appopriate approach by the look of the plot.  

### 2.2
A Probabilistic model...
  
### 2.3
The data set is divided into a training set and a validation set where each set contains 50 percent of the observations.  

```{r, echo=FALSE}
n=dim(tecator)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=tecator[id,]
test=tecator[-id,]

# Extract data sets for moisture and protein data from train and test
Protein_train <- train$Protein
Moisture_train <- train$Moisture
Protein_test <- test$Protein
Moisture_test <- test$Moisture

poly_modelTrain <- list()
MSE_train <- 0
MSE_test <- 0

for (i in 1:6){
poly_modelTrain[[i]] <-  lm(Moisture_train ~ poly(Protein_train, i, raw=TRUE))
MSE_train[i] <- 1/length(Moisture_train) * sum(poly_modelTrain[[i]]$residuals^2)
y_hat_test <- predict(lm(Moisture_train ~ poly(Protein_train, i)), data.frame(Protein_train=Protein_test))
MSE_test[i] <- 1/length(Moisture_test)  * sum((y_hat_test-Moisture_test)^2)
}


plot(1:6, MSE_train, type="l", ylim=c(30,35), col="blue")
lines(1:6, MSE_test, type="l", col="red")
```


### 2.4
Now the whole data set is used to construct the same models. To AIC values is computed to evaluate the models and decide which model that is the best one.  
```{r, echo=FALSE}
AIC <- 0
for (i in 1:6){
  poly_modelTrain[[i]] <-  lm(tecator$Moisture ~ poly(tecator$Protein, i, raw=TRUE))
  n <- length(tecator$Moisture)
  RSS <- sum(poly_modelTrain[[i]]$residuals^2)  
  AIC[i] <- 2*(i+1) + n * log(RSS/n)
}
AIC
```


### 2.5
Another area where the AIC criterion can be useful is in the variable selection. Here the function *stepAIC* starts with a linear model that has *fat* as response variable and *channel1-channel100* as predictors.  it is concluded that 63 of the predictors are selected.  

### 2.6 - 2.7
With the same response and predictor variables as in 2.5 a ridge regression model is fitted. The difference between a ridge regression model and a linear model is the use of a penalty factor $\lambda$ in the former model. The value of $\lambda$ affects the coefficients in such a way that they shrinks. How the values of the coefficients in the ridge regression model depends on the log of $\lambda$ is illustrated by plotting  these values.  

```{r, echo=FALSE, message=FALSE}
library(glmnet)
vars <- data.frame(tecator[,2:102])
# Create matrix with x variables 
mat_vars <- as.matrix(vars[,1:100])
# y variable
mat_y <- as.matrix(vars[,101])

ridge_mod <- glmnet(mat_vars, mat_y, alpha=0, family = "gaussian")
plot(ridge_mod, xvar="lambda",label=TRUE)
```

When fitting a LASSO model instead the dependence between the coefficients and log of $\lambda$ changes.  
```{r, echo=FALSE}
lasso_mod <- glmnet(mat_vars, mat_y, alpha=1, family = "gaussian")
plot(lasso_mod, xvar="lambda",label=TRUE)
```

### 2.8
To find a optimal LASSO model a possible approach is to use cross-validation  

```{r, echo=FALSE}
set.seed(12345)
lasso_cv <- cv.glmnet(mat_vars, mat_y, alpha=1, family = "gaussian")
plot(lasso_cv)
```

### 2.9
The variable selection done with *stepAIC* and the selection done with the cross-validated LASSO model choose a significantly different amount of variables. With *stepAIC* a model with 63 variables were obtained and from the cross-validated LASSO computations a model with 14 variables. 




